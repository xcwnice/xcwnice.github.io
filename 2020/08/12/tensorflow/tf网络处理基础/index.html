<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>tensorflow2.0与深度学习入门 | findway</title><meta name="keywords" content="神经网络,深度学习入门,tensorflow"><meta name="author" content="xcw"><meta name="copyright" content="xcw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="全连接层 Layers:  input hidden output  Heroes:  BigDATA ReLU Dropout BatchNorm ResNet XAVIR Initialization Caffe&#x2F;Tensorflow&#x2F;PyTorch   tf.keras.layers.Dense(units,activation) layer.Dense类:   .build(num)方法">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflow2.0与深度学习入门">
<meta property="og:url" content="http://xcwnice.gitee.io/2020/08/12/tensorflow/tf%E7%BD%91%E7%BB%9C%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="findway">
<meta property="og:description" content="全连接层 Layers:  input hidden output  Heroes:  BigDATA ReLU Dropout BatchNorm ResNet XAVIR Initialization Caffe&#x2F;Tensorflow&#x2F;PyTorch   tf.keras.layers.Dense(units,activation) layer.Dense类:   .build(num)方法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://xcwnice.gitee.io/img/xcw/find1.jpg">
<meta property="article:published_time" content="2020-08-12T02:59:34.000Z">
<meta property="article:modified_time" content="2021-03-15T02:10:44.483Z">
<meta property="article:author" content="xcw">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="深度学习入门">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://xcwnice.gitee.io/img/xcw/find1.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://xcwnice.gitee.io/2020/08/12/tensorflow/tf%E7%BD%91%E7%BB%9C%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: xcw","link":"链接: ","source":"来源: findway","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-03-15 10:10:44'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="findway" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/xcw/header1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">46</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">16</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/xcw/find1.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">findway</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">tensorflow2.0与深度学习入门</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-08-12T02:59:34.000Z" title="发表于 2020-08-12 10:59:34">2020-08-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-03-15T02:10:44.483Z" title="更新于 2021-03-15 10:10:44">2021-03-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/tensorflow%E5%85%A5%E9%97%A8/">tensorflow入门</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>全连接层</h1>
<p>Layers:</p>
<ul>
<li>input</li>
<li>hidden</li>
<li>output</li>
</ul>
<p>Heroes:</p>
<ul>
<li>BigDATA</li>
<li>ReLU</li>
<li>Dropout</li>
<li>BatchNorm</li>
<li>ResNet</li>
<li>XAVIR Initialization</li>
<li>Caffe/Tensorflow/PyTorch</li>
</ul>
<hr>
<h2 id="tf-keras-layers-Dense-units-activation">tf.keras.layers.Dense(units,activation)</h2>
<p>layer.Dense类:</p>
<ul>
<li>
<p>.build(num)方法</p>
<blockquote>
<p>num为输入节点数</p>
</blockquote>
</li>
<li>
<p>.kernel方法</p>
<blockquote>
<p>权值张量𝑾</p>
</blockquote>
</li>
<li>
<p>.bias方法</p>
<blockquote>
<p>偏置张量𝒃</p>
</blockquote>
</li>
<li>
<p>.trainable_variables属性</p>
<blockquote>
<p>待优化参数列表</p>
</blockquote>
</li>
<li>
<p>.non_trainable_variables属性</p>
<blockquote>
<p>所有不需要优化的参数列表</p>
</blockquote>
</li>
<li>
<p>.trainable属性</p>
<blockquote>
<p>所有内部张量列表</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>通过layer.Dense 类，只需要指定<strong>输出</strong>节点数Units 和激活函数类型activation 即可。需要注意的是，输入节点数会根据第一次运算时的输入shape 确定，同时根据输入、输出节点数自动创建并初始化权值张量𝑾和偏置张量𝒃，因此在新建类Dense 实例时，并不会立即创建权值张量𝑾和偏置张量𝒃，而是需要调用build 函数或者直接进行一次前向计算，才能完成网络参数的创建。其中activation 参数指定当前层的激活函数，可以为常见的激活函数或自定义激活函数，也可以指定为None，即无激活函数。</p>
</blockquote>
<p>我们可以通过类内部的成员名kernel 和bias 来获取权值张量𝑾和偏置张量𝒃对象:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x=tf.random.normal([<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">f=tf.keras.layers.Dense(<span class="number">10.</span>activation=tf.nn.relu)</span><br><span class="line">out=f(x)<span class="comment"># 用f类实例完成一层全连接层的计算</span></span><br><span class="line">f.kernel <span class="comment">#获取w</span></span><br><span class="line">f.bias <span class="comment">#获取b</span></span><br></pre></td></tr></table></figure>
<hr>
<p>机制:</p>
<blockquote>
<p>利用<strong>网络层类对象</strong>进行前向计算时，只需要调用类的__call__方法即可，即写成f(x)方式便可，它会自动调用类的__call__方法，在__call__方法中会自动调用call 方法，这一设定由TensorFlow 框架自动完成，因此用户只需要将网络层的前向计算逻辑实现在call 方法中即可。对于全连接层类，在call 方法中实现𝜎(𝑿@𝑾 + 𝒃)的运算逻辑，非常简单，最后返回全连接层的输出张量即可。</p>
</blockquote>
<hr>
<h2 id="神经网络">神经网络</h2>
<h3 id="tf-keras-Sequential-layer1-layer2-…">tf.keras.Sequential([layer1,layer2,…])</h3>
<blockquote>
<p>可以将每一个层封装起来,调用大类的前向计算函数一次即可完成所有层的前向计算.</p>
</blockquote>
<ul>
<li>
<p>.build(num)方法</p>
<blockquote>
<p>使用方法同上</p>
</blockquote>
</li>
<li>
<p>.summary( )方法</p>
<blockquote>
<p>可以打印出每层的参数列表</p>
</blockquote>
</li>
<li>
<p>call方法</p>
<blockquote>
<p>实现大类的向前运算，可以直接用model(x)实现</p>
<blockquote>
<p>在假定model=tf.keras.Sequential([layer1,layer2,…])时</p>
</blockquote>
</blockquote>
</li>
</ul>
<hr>
<p>实现代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,Sequential</span><br><span class="line">model=Sequential([</span><br><span class="line">    layers.Dense(<span class="number">20</span>,activation=tf.nn.relu),</span><br><span class="line">    layers.Dense(<span class="number">180</span>,activation=tf.nn.relu),</span><br><span class="line">    layers.Dense(<span class="number">10</span>,activation=tf.nn.relu),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">x=tf.random.normal([<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">out=model(x)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h1>激活函数</h1>
<h2 id="常见的激活函数">常见的激活函数</h2>
<h3 id="Sigmoid函数">Sigmoid函数</h3>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">y=\frac{1}{1+e^{-x}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.09077em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<blockquote>
<p>它的一个优良特性就是能够把𝑥 ∈ 𝑅的输入“压缩”到𝑥 ∈ (0,1)区间，这个区间的数值在机<br>
器学习常用来表示以下意义：</p>
<ul>
<li>概率分布 (0,1)区间的输出和概率的分布范围[0,1]契合，可以通过Sigmoid 函数将输出<br>
转译为概率输出</li>
<li>信号强度 一般可以将0~1 理解为某种信号的强度，如像素的颜色强度，1 代表当前通<br>
道颜色最强，0 代表当前通道无颜色；抑或代表门控值(Gate)的强度，1 代表当前门控<br>
全部开放,0代表关闭</li>
</ul>
</blockquote>
<ul>
<li>tf.nn.sigmoid(x)</li>
</ul>
<hr>
<h3 id="ReLU函数">ReLU函数</h3>
<p>公式:<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>𝑥</mi><mo stretchy="false">)</mo><mo>≜</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>𝑥</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">ReLU(𝑥) ≜ max(0, 𝑥)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1666699999999999em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10903em;">LU</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">≜</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></p>
<ul>
<li>tf.nn.relu(x)</li>
</ul>
<hr>
<h3 id="LeakyReLU">LeakyReLU</h3>
LeakyReLU ≜ \cases{x\quad(x\geq0)\\px\quad(x<0)} <ul>
<li>tf.nn.leaky_relu(x, alpha= )</li>

<hr>
<h3 id="Tanh函数">Tanh函数</h3>
<blockquote>
<p>Tanh 函数能够将𝑥 ∈ 𝑅的输入“压缩”到(−1,1)区间</p>
</blockquote>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">anh</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.217661em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.448331em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.590392em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<ul>
<li>tf.nn.tanh(x)</li>
</ul>
<hr>
<h2 id="输出层函数">输出层函数</h2>
<h3 id="恒等函数">恒等函数</h3>
<p>tf不对输出层处理即可</p>
<blockquote>
<p>回归问题</p>
</blockquote>
<hr>
<h3 id="softmax函数">softmax函数</h3>
<blockquote>
<p>分类问题</p>
</blockquote>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>𝑆</mi><mi>𝑜</mi><mi>𝑓</mi><mi>𝑡</mi><mi>𝑚</mi><mi>𝑥</mi><mo stretchy="false">(</mo><mi>𝑧</mi><mi>𝑖</mi><mo stretchy="false">)</mo><mo>≜</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></munderover><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">𝑆𝑜𝑓𝑡𝑚 𝑥(𝑧𝑖) ≜ \frac{e^{z_i}}{\sum_{j=1}^{d_{out}}e^{z_j}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1666699999999999em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">≜</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.656218em;vertical-align:-1.314826em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.341392em;"><span style="top:-2.120992em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.989008em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6064620000000001em;"><span style="top:-3.0050700000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.04398em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.04398em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.314826em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<ul>
<li>tf.nn.softmax</li>
</ul>
<blockquote>
<p>在 Softmax 函数的数值计算过程中，容易因输入值偏大发生数值溢出现象；在计算交叉熵时，也会出现数值溢出的问题。为了数值计算的稳定性，TensorFlow 中提供了一个统一的接口，将Softmax 与交叉熵损失函数同时实现，同时也处理了数值不稳定的异常，一般推荐使用这些接口函数，避免分开使用Softmax 函数与交叉熵损失函数。</p>
<p>函数式接口为tf.keras.losses.categorical_crossentropy(y_true,y_pred,from_logits=False)，其中y_true 代表了One-hot 编码后的真实标签，y_pred 表示网络的预测值，当from_logits 设置为True 时，y_pred 表示须为未经过Softmax 函数的变量z；</p>
<p>当from_logits 设置为False 时，y_pred 表示为经过Softmax 函数的输出。为了数值计算稳定性，一般设置from_logits 为True，此时tf.keras.losses.categorical_crossentropy 将在内部进行Softmax 函数计算，所以不需要在模型中显式调用Softmax 函数</p>
</blockquote>
<ul>
<li>tf.keras.losses.categorical_crossentropy(y_true,y_pred,from_logits=False)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">z = tf.random.normal([<span class="number">2</span>,<span class="number">10</span>]) <span class="comment"># 构造输出层的输出</span></span><br><span class="line">print(z.shape)</span><br><span class="line">y_onehot = tf.constant([<span class="number">1</span>,<span class="number">3</span>]) <span class="comment"># 构造真实值</span></span><br><span class="line">y_onehot = tf.one_hot(y_onehot, depth=<span class="number">10</span>) <span class="comment"># one-hot 编码</span></span><br><span class="line">print(y_onehot.shape)</span><br><span class="line"><span class="comment"># 输出层未使用Softmax 函数，故from_logits 设置为True</span></span><br><span class="line"><span class="comment"># 这样categorical_crossentropy 函数在计算损失函数前，会先内部调用Softmax 函数</span></span><br><span class="line">loss = keras.losses.categorical_crossentropy(y_onehot,z,from_logits=<span class="literal">True</span>)</span><br><span class="line">loss = tf.reduce_mean(loss) <span class="comment"># 计算平均交叉熵损失</span></span><br><span class="line">print(loss)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>6.5 输出层设计<br>
我们来特别地讨论网络的最后一层的设计，它除了和所有的隐藏层一样，完成维度变换、特征提取的功能，还作为输出层使用，需要根据具体的任务场景来决定是否使用激活函数，以及使用什么类型的激活函数等。<br>
我们将根据输出值的区间范围来分类讨论。常见的几种输出类型包括：<br>
❑ 𝑜𝑖 ∈ 𝑅𝑑 输出属于整个实数空间，或者某段普通的实数空间，比如函数值趋势的预<br>
测，年龄的预测问题等。<br>
❑ 𝑜𝑖 ∈ [0,1] 输出值特别地落在[0, 1]的区间，如图片生成，图片像素值一般用[0, 1]区间<br>
的值表示；或者二分类问题的概率，如硬币正反面的概率预测问题。<br>
❑ 𝑜𝑖 ∈ [0, 1], 𝑖 𝑜𝑖 = 1 输出值落在[0,1]的区间，并且所有输出值之和为 1，常见的如<br>
多分类问题，如MNIST 手写数字图片识别，图片属于10 个类别的概率之和应为1。<br>
❑ 𝑜𝑖 ∈ [−1, 1] 输出值在[-1, 1]之间</p>
</blockquote>
<hr>
<h1>误差分析</h1>
<p>Outline:</p>
<ul>
<li>MSE</li>
<li>Cross Entopy Loss</li>
<li>Hinge Loss</li>
</ul>
<hr>
<h2 id="MSE">MSE</h2>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>∑</mo><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mi>o</mi><mi>u</mi><mi>t</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">loss=\frac{1}{N}\sum(y-out)^2
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">oss</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p>
<ul>
<li>tf.losses.MSE(y_true,y_pred)</li>
</ul>
<blockquote>
<p>同时也与tf.reduce_mean( )搭配使用，求出平均误差</p>
</blockquote>
<h2 id="Cross-Entopy-Loss">Cross Entopy Loss</h2>
<ul>
<li>tf.keras.losses.categorical_crossentropy(y_true,y_pred,from_logits=False)</li>
</ul>
<blockquote>
<p>对y_true一定要one_hot化</p>
<p>交叉熵使用的更多</p>
</blockquote>
<h1>反向传播</h1>
<h2 id="tf-GradientTape">tf.GradientTape( )</h2>
<p>tensorflow有自动求导的功能，如果遇到一定要手动求导的问题，也可以自己构建计算图的方式来求导</p>
<ul>
<li>
<p>tf.GradientTape()可以提供求导的上下文管理器来连接需要计算梯度的函数和变量，一般与with as语句连用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x=tf.constant(<span class="number">3.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    tape.watch(x)</span><br><span class="line">    y=x*x*x</span><br><span class="line">dy_dx=tape.gradient(y,[x])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tf.GradientTape(persistent=True,watch_accessed_variables=True)</p>
<ul>
<li><strong>persistent:</strong> 布尔值，用来指定新创建的gradient tape是否是可持续性的。默认是False，意味着只能够调用一次<code>gradient（）</code>函数。</li>
<li><strong>watch_accessed_variables:</strong> 布尔值，表明这个gradien tap是不是会自动追踪任何能被训练（trainable）的变量。默认是True。要是为False的话，意味着你需要手动去指定你想追踪的那些变量。</li>
</ul>
<blockquote>
<p>但tf.GradientTape只能默认追踪Variable变量，如果是constant变量则需要用.watch( )方法来添加为可追踪变量</p>
</blockquote>
</blockquote>
</li>
</ul>
<h2 id="Himmelblau函数优化练习">Himmelblau函数优化练习</h2>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mi>y</mi><mo>−</mo><mn>11</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><msup><mi>y</mi><mn>2</mn></msup><mo>−</mo><mn>7</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(x,y)=(x^2+y-11)^2+(x+y^2-7)^2
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord">11</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.0585479999999998em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord">7</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p>
<blockquote>
<p>Himmelblau 函数是用来测试优化算法的常用样例函数之一</p>
</blockquote>
<p>用python代码来表示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Himmelblau</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (x[<span class="number">0</span>]**<span class="number">2</span>+x[<span class="number">1</span>]-<span class="number">11</span>)**<span class="number">2</span>+(x[<span class="number">0</span>]+x[<span class="number">1</span>]**<span class="number">2</span>-<span class="number">7</span>)**<span class="number">2</span>    </span><br></pre></td></tr></table></figure>
<p>用Matplotlib 库可视化Himmelblau 函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(-<span class="number">6</span>, <span class="number">6</span>, <span class="number">0.1</span>) <span class="comment"># 可视化的x 坐标范围为-6~6</span></span><br><span class="line">y = np.arange(-<span class="number">6</span>, <span class="number">6</span>, <span class="number">0.1</span>) <span class="comment"># 可视化的y 坐标范围为-6~6</span></span><br><span class="line">print(<span class="string">&#x27;x,y range:&#x27;</span>, x.shape, y.shape)</span><br><span class="line"><span class="comment"># 生成x-y 平面采样网格点，方便可视化</span></span><br><span class="line">x, y = np.meshgrid(x, y)</span><br><span class="line">print(<span class="string">&#x27;X,Y maps:&#x27;</span>, X.shape, Y.shape)</span><br><span class="line">z = Himmelblau([x, y]) <span class="comment"># 计算网格点上的函数值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line">fig = plt.figure(<span class="string">&#x27;himmelblau&#x27;</span>)</span><br><span class="line">ax = fig.gca(projection=<span class="string">&#x27;3d&#x27;</span>) <span class="comment"># 设置3D 坐标轴</span></span><br><span class="line">ax.plot_surface(x, y, z) <span class="comment"># 3D 曲面图</span></span><br><span class="line">ax.view_init(<span class="number">60</span>, -<span class="number">30</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>自动求导:</p>
<ul>
<li>tf.constant版</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数的初始化值对优化的影响不容忽视，可以通过尝试不同的初始化值，</span></span><br><span class="line"><span class="comment"># 检验函数优化的极小值情况</span></span><br><span class="line"><span class="comment"># [1., 0.], [-4, 0.], [4, 0.]</span></span><br><span class="line">x = tf.constant([<span class="number">4.</span>, <span class="number">0.</span>]) <span class="comment"># 初始化参数</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):<span class="comment"># 循环优化200 次</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment">#梯度跟踪</span></span><br><span class="line">        tape.watch([x]) <span class="comment"># 加入梯度跟踪列表</span></span><br><span class="line">        y = himmelblau(x) <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    grads = tape.gradient(y, [x])[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 更新参数,0.01 为学习率</span></span><br><span class="line">    x -= <span class="number">0.01</span>*grads</span><br><span class="line">    <span class="comment"># 打印优化的极小值</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">19</span>:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;step &#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(step, x.numpy(), y.numpy()))</span><br></pre></td></tr></table></figure>
<ul>
<li>tf.Variable版</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数的初始化值对优化的影响不容忽视，可以通过尝试不同的初始化值，</span></span><br><span class="line"><span class="comment"># 检验函数优化的极小值情况</span></span><br><span class="line"><span class="comment"># [1., 0.], [-4, 0.], [4, 0.]</span></span><br><span class="line">x = tf.Variable([<span class="number">4.</span>, <span class="number">0.</span>]) <span class="comment"># 初始化参数</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):<span class="comment"># 循环优化200 次</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment">#梯度跟踪</span></span><br><span class="line">        tape.watch([x]) <span class="comment"># 加入梯度跟踪列表</span></span><br><span class="line">        y = Himmelblau(x) <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    grads = tape.gradient(y,[x,])</span><br><span class="line">    <span class="comment"># 更新参数,0.01 为学习率</span></span><br><span class="line">    x.assign_sub(<span class="number">0.01</span>*grads[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 打印优化的极小值</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">19</span>:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;step &#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(step, x.numpy(), y.numpy()))</span><br></pre></td></tr></table></figure>
<h1>Keras API for tensorflow</h1>
<h2 id="网络层类">网络层类</h2>
<h3 id="常见内置模型层介绍">常见内置模型层介绍</h3>
<p>tf.keras.layers下</p>
<p><strong>基础层</strong></p>
<ul>
<li>Dense：密集连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)</li>
<li>Activation：激活函数层。一般放在Dense层后面，等价于在Dense层中指定activation。</li>
<li>Dropout：随机置零层。训练期间以一定几率将输入置0，一种正则化手段。</li>
<li>BatchNormalization：批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。</li>
<li>SpatialDropout2D：空间随机置零层。训练期间以一定几率将整个特征图置0，一种正则化手段，有利于避免特征图之间过高的相关性。</li>
<li>Input：输入层。通常使用Functional API方式构建模型时作为第一层。</li>
<li>DenseFeature：特征列接入层，用于接收一个特征列列表并产生一个密集连接层。</li>
<li>Flatten：压平层，用于将多维张量压成一维。</li>
<li>Reshape：形状重塑层，改变输入张量的形状。</li>
<li>Concatenate：拼接层，将多个张量在某个维度上拼接。</li>
<li>Add：加法层。</li>
<li>Subtract： 减法层。</li>
<li>Maximum：取最大值层。</li>
<li>Minimum：取最小值层。</li>
</ul>
<p><strong>卷积网络相关层</strong></p>
<ul>
<li>Conv1D：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数</li>
<li>Conv2D：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数</li>
<li>Conv3D：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数</li>
<li>SeparableConv2D：二维深度可分离卷积层。不同于普通卷积同时对区域和通道操作，深度可分离卷积先操作区域，再操作通道。即先对每个通道做独立卷积操作区域，再用1乘1卷积跨通道组合操作通道。参数个数 = 输入通道数×卷积核尺寸 + 输入通道数×1×1×输出通道数。深度可分离卷积的参数数量一般远小于普通卷积，效果一般也更好。</li>
<li>DepthwiseConv2D：二维深度卷积层。仅有SeparableConv2D前半部分操作，即只操作区域，不操作通道，一般输出通道数和输入通道数相同，但也可以通过设置depth_multiplier让输出通道为输入通道的若干倍数。输出通道数 = 输入通道数 × depth_multiplier。参数个数 = 输入通道数×卷积核尺寸× depth_multiplier。</li>
<li>Conv2DTranspose：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。</li>
<li>LocallyConnected2D: 二维局部连接层。类似Conv2D，唯一的差别是没有空间上的权值共享，所以其参数个数远高于二维卷积。</li>
<li>MaxPooling2D: 二维最大池化层。也称作下采样层。池化层无参数，主要作用是降维。</li>
<li>AveragePooling2D: 二维平均池化层。</li>
<li>GlobalMaxPool2D: 全局最大池化层。每个通道仅保留一个值。一般从卷积层过渡到全连接层时使用，是Flatten的替代方案。</li>
<li>GlobalAvgPool2D: 全局平均池化层。每个通道仅保留一个值。</li>
</ul>
<p><strong>循环网络相关层</strong></p>
<ul>
<li>Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</li>
<li>LSTM：长短记忆循环网络层。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置return_sequences = True时可以返回各个中间步骤输出，否则只返回最终输出。</li>
<li>GRU：门控循环网络层。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。</li>
<li>SimpleRNN：简单循环网络层。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。</li>
<li>ConvLSTM2D：卷积长短记忆循环网络层。结构上类似LSTM，但对输入的转换操作和对状态的转换操作都是卷积运算。</li>
<li>Bidirectional：双向循环网络包装器。可以将LSTM，GRU等层包装成双向循环网络。从而增强特征提取能力。</li>
<li>RNN：RNN基本层。接受一个循环网络单元或一个循环单元列表，通过调用tf.keras.backend.rnn函数在序列上进行迭代从而转换成循环网络层。</li>
<li>LSTMCell：LSTM单元。和LSTM在整个序列上迭代相比，它仅在序列上迭代一步。可以简单理解LSTM即RNN基本层包裹LSTMCell。</li>
<li>GRUCell：GRU单元。和GRU在整个序列上迭代相比，它仅在序列上迭代一步。</li>
<li>SimpleRNNCell：SimpleRNN单元。和SimpleRNN在整个序列上迭代相比，它仅在序列上迭代一步。</li>
<li>AbstractRNNCell：抽象RNN单元。通过对它的子类化用户可以自定义RNN单元，再通过RNN基本层的包裹实现用户自定义循环网络层。</li>
<li>Attention：Dot-product类型注意力机制层。可以用于构建注意力模型。</li>
<li>AdditiveAttention：Additive类型注意力机制层。可以用于构建注意力模型。</li>
<li>TimeDistributed：时间分布包装器。包装后可以将Dense、Conv2D等作用到每一个时间片段上。</li>
</ul>
<h3 id="自定义层">自定义层</h3>
<p>对于自定义的网络层，至少需要实现初始化__init__方法和前向传播逻辑<strong>call</strong>方法。</p>
<p>如一个没有偏置向量的全连接层:即bias 为0，同时固定激活函数为ReLU 函数。</p>
<p>实现代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span>(<span class="params">layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,inp_dim,outp_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyDense,self).__init__()</span><br><span class="line">        self.kernel=self.add_variable(<span class="string">&#x27;w&#x27;</span>,[inp_dim,outp_dim],trainable=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self,inputs,training=<span class="literal">None</span></span>):</span></span><br><span class="line">        out=inputs@self.kernel</span><br><span class="line">        out=tf.nn.relu(out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>自定义类的前向运算逻辑实现在call(inputs, training=None)函数中，其中inputs代表输入，由用户在调用时传入；training 参数用于指定模型的状态：training 为True 时执行训练模式，training 为False 时执行测试模式，默认参数为None，即测试模式。由于全连接层的训练模式和测试模式逻辑一致，此处不需要额外处理。对于部份测试模式和训练模式不一致的网络层，需要根据training 参数来设计需要执行的逻辑。</p>
</blockquote>
<h2 id="网络类">网络类</h2>
<h3 id="内置网络容器Sequential">内置网络容器Sequential</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Sequential</span><br><span class="line">network = Sequential([ <span class="comment"># 封装为一个网络</span></span><br><span class="line">    layers.Dense(<span class="number">3</span>, activation=<span class="literal">None</span>), <span class="comment"># 全连接层，此处不使用激活函数</span></span><br><span class="line">    layers.ReLU(),<span class="comment">#激活函数层</span></span><br><span class="line">    layers.Dense(<span class="number">2</span>, activation=<span class="literal">None</span>), <span class="comment"># 全连接层，此处不使用激活函数</span></span><br><span class="line">    layers.ReLU() <span class="comment">#激活函数层</span></span><br><span class="line">])</span><br><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">out = network(x) <span class="comment"># 输入从第一层开始，逐层传播至输出层，并返回输出层的输出</span></span><br></pre></td></tr></table></figure>
<p>Sequential 容器也可以通过add()方法继续追加新的网络层，实现动态创建网络的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Sequential</span><br><span class="line">layers_num = <span class="number">2</span> <span class="comment"># 堆叠2 次</span></span><br><span class="line">network = Sequential([]) <span class="comment"># 先创建空的网络容器</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(layers_num):</span><br><span class="line">    network.add(layers.Dense(<span class="number">3</span>)) <span class="comment"># 添加全连接层</span></span><br><span class="line">    network.add(layers.ReLU())<span class="comment"># 添加激活函数层</span></span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">4</span>)) <span class="comment"># 创建网络参数</span></span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure>
<h3 id="模型装配、训练与测试">模型装配、训练与测试</h3>
<blockquote>
<p>假定有一个全连接模型:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Sequential,optimizers,losses</span><br><span class="line">network = Sequential([</span><br><span class="line">    layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">10</span>)</span><br><span class="line">])</span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure>
</blockquote>
<p>在keras.model类中</p>
<ul>
<li>
<p>模型装配</p>
<ul>
<li>
<p>.compile( )方法</p>
<blockquote>
<p>可以指定网络使用的优化器对象，损失函数类型，评价指标</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用Adam 优化器，学习率为0.01;采用交叉熵损失函数，包含Softmax</span></span><br><span class="line">network.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=optimizers.Adam(lr=<span class="number">0.01</span>),</span><br><span class="line">    loss=losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>] <span class="comment"># 设置测量指标为准确率</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>模型训练</p>
<ul>
<li>
<p>.fit( )方法</p>
<blockquote>
<p>模型装配完成后，即可通过fit()函数送入待训练的数据集和验证用的数据集,进行训练</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定训练集为train_db，验证集为val_db,训练5 个epochs，每2 个epoch 验证一次</span></span><br><span class="line"><span class="comment"># 返回训练轨迹信息保存在history 对象中</span></span><br><span class="line">history = network.fit(train_db, epochs=<span class="number">5</span>, validation_data=val_db,</span><br><span class="line">validation_freq=<span class="number">2</span>,callback=[tensorboard])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>模型测试，推理</p>
<ul>
<li>
<p>.predict( )方法</p>
<blockquote>
<p>可以进行模型推理，并返回预测结果</p>
</blockquote>
</li>
<li>
<p>.evaluate( )方法</p>
<blockquote>
<p>可以直接测试指定数据集所有样本，并打印出性能指标</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="自定义网络">自定义网络</h3>
<blockquote>
<p>Sequential 容器适合于数据按序从第一层传播到第二层，再从第二层传播到第三层，以<br>
此规律传播的网络模型。对于复杂的网络结构，例如第三层的输入不仅是第二层的输出，<br>
还有第一层的输出，此时使用自定义网络更加灵活。</p>
</blockquote>
<ul>
<li>网络层的创建</li>
<li>前向运算逻辑</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line"><span class="comment"># 自定义网络类，继承自Model 基类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">    <span class="comment"># 完成网络内需要的网络层的创建工作</span></span><br><span class="line">    self.fc1 = MyDense(<span class="number">28</span>*<span class="number">28</span>, <span class="number">256</span>)</span><br><span class="line">    self.fc2 = MyDense(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">    self.fc3 = MyDense(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">    self.fc4 = MyDense(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">    self.fc5 = MyDense(<span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 自定义前向运算逻辑</span></span><br><span class="line">    x = self.fc1(inputs)</span><br><span class="line">    x = self.fc2(x)</span><br><span class="line">    x = self.fc3(x)</span><br><span class="line">    x = self.fc4(x)</span><br><span class="line">    x = self.fc5(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="模型保存和加载">模型保存和加载</h2>
<ul>
<li>
<p>张量方式</p>
<ul>
<li>
<p><strong>Model.save_weights(path)</strong></p>
<blockquote>
<p>通过调用**Model.save_weights(path)**方法即可将当前的网络参数保存到path 文件上，</p>
</blockquote>
</li>
<li>
<p><strong>Model.load_weights(path)</strong></p>
<blockquote>
<p>然后调用网络对象的**load_weights(path)**方法即可将指定的模型文件中保存的张量数值写入<br>
到当前网络参数中去</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>网络方式</p>
<ul>
<li>
<p>Model.save(path)</p>
<blockquote>
<p>将模型的参数和结构都保存到path上，一般以h5为文件后缀</p>
</blockquote>
</li>
<li>
<p>tf.keras.models.load_model(file)</p>
<blockquote>
<p>加载已保存的模型文件( 用Model.save( ) 和tf.saved_model.save( ) )</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p><strong>savemodel方式</strong></p>
<blockquote>
<p>将网络及参数保存为pb格式，方便其他工具读取，如pytorch,Netron, 也方便跨平台使用</p>
</blockquote>
<ul>
<li>
<p>tf.saved_model.save(model,path)</p>
<blockquote>
<p>保存模型与参数数据</p>
</blockquote>
</li>
<li>
<p>tf.saved_model.load(path)</p>
<blockquote>
<p>加载模型</p>
<blockquote>
<p><em>Loading Keras models</em></p>
<p>Keras models are trackable, so they can be saved to SavedModel. The object returned by <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/saved_model/load"><code>tf.saved_model.load</code></a> is not a Keras object (i.e. doesn’t have <code>.fit</code>, <code>.predict</code>, etc. methods). A few attributes and functions are still available: <code>.variables</code>, <code>.trainable_variables</code> and <code>.__call__</code>.</p>
</blockquote>
<p>所以我们如果要加载回keras模型可以用回tf.keras.models.load_model(file)</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h2 id="加载模型">加载模型</h2>
<p>在keras.application中有多种在ImageNet上训练好参数的模型，可以直接调用</p>
<p>如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">resnet=keras.application.Resnet50(weight=<span class="string">&#x27;imagenet&#x27;</span>,include_top=false)</span><br></pre></td></tr></table></figure>
<h2 id="测量工具">测量工具</h2>
<p>keras。。。</p>
<h2 id="可视化">可视化</h2>
<blockquote>
<p>关键工具TensorBoard和关键模块tf.summary或来自tf.keras.model模块中的callback里tf.keras.callbacks.TensorBoard</p>
</blockquote>
<hr>
<p>在cmd中使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir&#x3D;PATH</span><br></pre></td></tr></table></figure>
<blockquote>
<p>就是保存log_dir的路径</p>
</blockquote>
<hr>
<h3 id="keras-Model类中fit方法使用callback">keras.Model类中fit方法使用callback</h3>
<p>Example(Basic):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=<span class="string">&quot;.\\logs&quot;</span>)</span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">2</span>, callbacks=[tensorboard_callback])</span><br><span class="line"><span class="comment"># run the tensorboard command to view the visualizations.</span></span><br></pre></td></tr></table></figure>
<p>Example(Profile):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># profile a single batch, e.g. the 5th batch.</span></span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=<span class="string">&#x27;.\\logs&#x27;</span>,profile_batch=<span class="number">5</span>)</span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">2</span>, callbacks=[tensorboard_callback])</span><br><span class="line"><span class="comment"># Now run the tensorboard command to view the visualizations (profile plugin).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># profile a range of batches, e.g. from 10 to 20.</span></span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=<span class="string">&#x27;.\\logs&#x27;</span>,profile_batch=<span class="string">&#x27;10,20&#x27;</span>)</span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">2</span>, callbacks=[tensorboard_callback])</span><br><span class="line"><span class="comment"># Now run the tensorboard command to view the visualizations (profile plugin).</span></span><br></pre></td></tr></table></figure>
<h3 id="关键模块tf-summary">关键模块tf.summary</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">summary_writer=tf.summary.create_file_writer(log_dir)</span><br><span class="line"><span class="keyword">with</span> summary_writer.as_dafault():</span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># other model code would go here</span></span><br><span class="line">    tf.summary.scalar(<span class="string">&quot;my_metric&quot;</span>, <span class="number">0.5</span>, step=step)</span><br><span class="line">    writer.flush()</span><br></pre></td></tr></table></figure>
</0)}></article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>tensorflow2.0与深度学习入门</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="http://xcwnice.gitee.io/2020/08/12/tensorflow/tf%E7%BD%91%E7%BB%9C%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80/">http://xcwnice.gitee.io/2020/08/12/tensorflow/tf%E7%BD%91%E7%BB%9C%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80/</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>xcw</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2020-08-12</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2021-03-15</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/">深度学习入门</a><a class="post-meta__tags" href="/tags/tensorflow/">tensorflow</a></div><div class="post_share"><div class="social-share" data-image="/img/xcw/find1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/08/12/tensorflow/tf%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80/"><img class="prev-cover" src="/img/xcw/find1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">tensorflow2.0基础与进阶</div></div></a></div><div class="next-post pull-right"><a href="/2020/08/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><img class="next-cover" src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=13114550,2928413146&amp;fm=26&amp;gp=0.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">迁移学习</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/09/19/tensorflow/tensorflow异常处理/" title="tensorflow2.0异常处理"><img class="cover" src="/img/xcw/find1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-19</div><div class="title">tensorflow2.0异常处理</div></div></a></div><div><a href="/2020/08/12/tensorflow/tf数据处理基础/" title="tensorflow2.0基础与进阶"><img class="cover" src="/img/xcw/find1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-12</div><div class="title">tensorflow2.0基础与进阶</div></div></a></div><div><a href="/2020/08/10/神经网络/卷积神经网络/" title="卷积神经网络"><img class="cover" src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=13114550,2928413146&fm=26&gp=0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-10</div><div class="title">卷积神经网络</div></div></a></div><div><a href="/2020/08/09/神经网络/与学习相关的技巧/" title="与学习相关的技巧"><img class="cover" src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=13114550,2928413146&fm=26&gp=0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-09</div><div class="title">与学习相关的技巧</div></div></a></div><div><a href="/2020/07/28/神经网络/感知机/" title="感知机"><img class="cover" src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1595764414107&di=93ee542045570b12290d1eb21dff2e30&imgtype=0&src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180319%2F66179109ed054505ba02b054d711dbd1.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-28</div><div class="title">感知机</div></div></a></div><div><a href="/2020/08/12/神经网络/改善深层神经网络：超参数调试、正则化以及优化/" title="改善深层神经网络：超参数调试、正则化以及优化"><img class="cover" src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1595764414107&di=93ee542045570b12290d1eb21dff2e30&imgtype=0&src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180319%2F66179109ed054505ba02b054d711dbd1.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-12</div><div class="title">改善深层神经网络：超参数调试、正则化以及优化</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/xcw/header1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">xcw</div><div class="author-info__description">欢迎每一位的到来</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">46</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">16</div></a></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">全连接层</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-keras-layers-Dense-units-activation"><span class="toc-number">1.1.</span> <span class="toc-text">tf.keras.layers.Dense(units,activation)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.2.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-keras-Sequential-layer1-layer2-%E2%80%A6"><span class="toc-number">1.2.1.</span> <span class="toc-text">tf.keras.Sequential([layer1,layer2,…])</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">常见的激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.1.</span> <span class="toc-text">Sigmoid函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.2.</span> <span class="toc-text">ReLU函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LeakyReLU"><span class="toc-number">2.1.3.</span> <span class="toc-text">LeakyReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tanh%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.4.</span> <span class="toc-text">Tanh函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">输出层函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%81%92%E7%AD%89%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.1.</span> <span class="toc-text">恒等函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.2.</span> <span class="toc-text">softmax函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">误差分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MSE"><span class="toc-number">3.1.</span> <span class="toc-text">MSE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-Entopy-Loss"><span class="toc-number">3.2.</span> <span class="toc-text">Cross Entopy Loss</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-GradientTape"><span class="toc-number">4.1.</span> <span class="toc-text">tf.GradientTape( )</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Himmelblau%E5%87%BD%E6%95%B0%E4%BC%98%E5%8C%96%E7%BB%83%E4%B9%A0"><span class="toc-number">4.2.</span> <span class="toc-text">Himmelblau函数优化练习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">Keras API for tensorflow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E5%B1%82%E7%B1%BB"><span class="toc-number">5.1.</span> <span class="toc-text">网络层类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E5%86%85%E7%BD%AE%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BB%8B%E7%BB%8D"><span class="toc-number">5.1.1.</span> <span class="toc-text">常见内置模型层介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="toc-number">5.1.2.</span> <span class="toc-text">自定义层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%B1%BB"><span class="toc-number">5.2.</span> <span class="toc-text">网络类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E7%BD%AE%E7%BD%91%E7%BB%9C%E5%AE%B9%E5%99%A8Sequential"><span class="toc-number">5.2.1.</span> <span class="toc-text">内置网络容器Sequential</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%A3%85%E9%85%8D%E3%80%81%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95"><span class="toc-number">5.2.2.</span> <span class="toc-text">模型装配、训练与测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C"><span class="toc-number">5.2.3.</span> <span class="toc-text">自定义网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD"><span class="toc-number">5.3.</span> <span class="toc-text">模型保存和加载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.4.</span> <span class="toc-text">加载模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E9%87%8F%E5%B7%A5%E5%85%B7"><span class="toc-number">5.5.</span> <span class="toc-text">测量工具</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">5.6.</span> <span class="toc-text">可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#keras-Model%E7%B1%BB%E4%B8%ADfit%E6%96%B9%E6%B3%95%E4%BD%BF%E7%94%A8callback"><span class="toc-number">5.6.1.</span> <span class="toc-text">keras.Model类中fit方法使用callback</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97tf-summary"><span class="toc-number">5.6.2.</span> <span class="toc-text">关键模块tf.summary</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/05/19/opencv/opencv_tf/" title="opencv-yolov3"><img src="/img/xcw/find1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="opencv-yolov3"/></a><div class="content"><a class="title" href="/2021/05/19/opencv/opencv_tf/" title="opencv-yolov3">opencv-yolov3</a><time datetime="2021-05-19T02:59:34.000Z" title="发表于 2021-05-19 10:59:34">2021-05-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E7%AE%97%E6%B3%95/" title="算法"><img src="/img/xcw/find1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="算法"/></a><div class="content"><a class="title" href="/2021/05/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E7%AE%97%E6%B3%95/" title="算法">算法</a><time datetime="2021-05-15T13:59:34.000Z" title="发表于 2021-05-15 21:59:34">2021-05-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%BB%AA%E8%AE%BA/" title="数据结构绪论"><img src="/img/xcw/find1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构绪论"/></a><div class="content"><a class="title" href="/2021/05/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%BB%AA%E8%AE%BA/" title="数据结构绪论">数据结构绪论</a><time datetime="2021-05-15T02:59:34.000Z" title="发表于 2021-05-15 10:59:34">2021-05-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/02/python/pandas%E5%AD%A6%E4%B9%A0/" title="pandas学习"><img src="/img/xcw/find1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="pandas学习"/></a><div class="content"><a class="title" href="/2021/05/02/python/pandas%E5%AD%A6%E4%B9%A0/" title="pandas学习">pandas学习</a><time datetime="2021-05-02T12:24:31.000Z" title="发表于 2021-05-02 20:24:31">2021-05-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/01/k210/k210/" title="k210零碎点"><img src="/img/xcw/find1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="k210零碎点"/></a><div class="content"><a class="title" href="/2021/05/01/k210/k210/" title="k210零碎点">k210零碎点</a><time datetime="2021-05-01T10:20:06.811Z" title="发表于 2021-05-01 18:20:06">2021-05-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By xcw</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><script async data-pjax src="/js/randombg.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>